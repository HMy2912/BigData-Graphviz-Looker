{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd715836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp, log\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../creditcard.csv/creditcard.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['Time', 'Class'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737e2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term (column of 1s)\n",
    "X = np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets (80/20 split)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[indices[:train_size]], X[indices[train_size:]]\n",
    "y_train, y_test = y[indices[:train_size]], y[indices[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3a4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (except intercept column)\n",
    "for i in range(1, X_train.shape[1]):\n",
    "    mean = X_train[:, i].mean()\n",
    "    std = X_train[:, i].std()\n",
    "    X_train[:, i] = (X_train[:, i] - mean) / std\n",
    "    X_test[:, i] = (X_test[:, i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee46480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, class_weight=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.class_weight = class_weight\n",
    "        self.weights = None\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _loss(self, h, y, sample_weights=None):\n",
    "        if sample_weights is None:\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        else:\n",
    "            return ((-y * np.log(h) - (1 - y) * np.log(1 - h)) * sample_weights).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Calculate class weights if specified\n",
    "        if self.class_weight == 'balanced':\n",
    "            n_samples = len(y)\n",
    "            n_classes = 2\n",
    "            class_counts = np.bincount(y)\n",
    "            self.class_weight_ = n_samples / (n_classes * class_counts)\n",
    "            sample_weights = np.where(y == 1, self.class_weight_[1], self.class_weight_[0])\n",
    "        else:\n",
    "            sample_weights = None\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            z = np.dot(X, self.weights)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            # Calculate gradient with class weights if specified\n",
    "            if sample_weights is not None:\n",
    "                gradient = np.dot(X.T, (h - y) * sample_weights) / len(y)\n",
    "            else:\n",
    "                gradient = np.dot(X.T, (h - y)) / len(y)\n",
    "                \n",
    "            self.weights -= self.learning_rate * gradient\n",
    "            \n",
    "            # Optional: Print loss every 100 iterations\n",
    "            # if _ % 100 == 0:\n",
    "            #     print(f'Loss at iteration {_}: {self._loss(h, y, sample_weights)}')\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self._sigmoid(np.dot(X, self.weights))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290a3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(X, y, k=5, ratio=1.0):\n",
    "    \"\"\"\n",
    "    SMOTE implementation for binary classification\n",
    "    \n",
    "    Parameters:\n",
    "    X - feature matrix\n",
    "    y - target vector\n",
    "    k - number of nearest neighbors to consider\n",
    "    ratio - sampling ratio (1.0 means equal classes)\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled, y_resampled\n",
    "    \"\"\"\n",
    "    # Separate minority and majority classes\n",
    "    minority_class = 1\n",
    "    majority_class = 0\n",
    "    X_min = X[y == minority_class]\n",
    "    X_maj = X[y == majority_class]\n",
    "    \n",
    "    n_minority = X_min.shape[0]\n",
    "    n_majority = X_maj.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Calculate how many synthetic samples to create\n",
    "    n_synthetic = int((ratio * n_majority) - n_minority)\n",
    "    \n",
    "    # Find k nearest neighbors for each minority sample\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 to exclude self\n",
    "    knn.fit(X_min)\n",
    "    distances, indices = knn.kneighbors(X_min)\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples = np.zeros((n_synthetic, n_features))\n",
    "    for i in range(n_synthetic):\n",
    "        # Randomly select a minority sample\n",
    "        idx = np.random.randint(0, n_minority)\n",
    "        # Randomly select one of its k nearest neighbors\n",
    "        neighbor_idx = np.random.choice(indices[idx, 1:])  # exclude self\n",
    "        # Generate synthetic sample\n",
    "        diff = X_min[neighbor_idx] - X_min[idx]\n",
    "        synthetic_samples[i] = X_min[idx] + np.random.random() * diff\n",
    "    \n",
    "    # Combine original minority with synthetic samples\n",
    "    X_resampled = np.vstack((X, synthetic_samples))\n",
    "    y_resampled = np.hstack((y, np.ones(n_synthetic)))\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b760b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y, ratio=1.0):\n",
    "    \"\"\"Random undersampling for binary classification\"\"\"\n",
    "    minority_class = 1\n",
    "    majority_class = 0\n",
    "\n",
    "    # Convert X and y to Pandas objects and reset index\n",
    "    X_df = pd.DataFrame(X).reset_index(drop=True)\n",
    "    y_series = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "    # Create boolean masks\n",
    "    minority_mask = y_series == minority_class\n",
    "    majority_mask = y_series == majority_class\n",
    "\n",
    "    # Select minority and majority samples\n",
    "    X_min, y_min = X_df[minority_mask], y_series[minority_mask]\n",
    "    X_maj, y_maj = X_df[majority_mask], y_series[majority_mask]\n",
    "\n",
    "    # Number of minority samples\n",
    "    n_minority = len(X_min)\n",
    "\n",
    "    # Number of majority samples to keep\n",
    "    n_majority_down = int(n_minority / ratio)\n",
    "\n",
    "    # Randomly sample majority class\n",
    "    np.random.seed(42)\n",
    "    X_maj_down = X_maj.sample(n=n_majority_down, random_state=42)\n",
    "    y_maj_down = y_maj.loc[X_maj_down.index]\n",
    "\n",
    "    # Combine undersampled majority with full minority class\n",
    "    X_resampled = pd.concat([X_min, X_maj_down]).reset_index(drop=True)\n",
    "    y_resampled = pd.concat([y_min, y_maj_down]).reset_index(drop=True)\n",
    "\n",
    "    return X_resampled.to_numpy(), y_resampled.to_numpy()  # Convert back to NumPy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707604bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Without SMOTE (using class weights)\n",
    "model = LogisticRegression(learning_rate=0.1, n_iterations=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26444ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\029at\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Option 2: With SMOTE\n",
    "X_train_smote, y_train_smote = smote(X_train, y_train, ratio=0.5)  # 50% minority class\n",
    "model_smote = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model_smote.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c55867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: With downsampling\n",
    "X_train_down, y_train_down = downsample(X_train, y_train, ratio=1.0)  # 1:1 ratio\n",
    "model_down = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model_down.fit(X_train_down, y_train_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d7aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def evaluate(y_true, y_pred):\n",
    "    from collections import defaultdict\n",
    "    metrics = defaultdict(float)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    metrics['confusion_matrix'] = np.array([[tn, fp], [fn, tp]])\n",
    "    metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd4ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "y_pred_down = model_down.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef108dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without SMOTE:\n",
      "Accuracy: 0.9743\n",
      "Precision: 0.0641\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.1202\n",
      "Confusion Matrix:\n",
      "[[55398  1459]\n",
      " [    5   100]]\n",
      "\n",
      "With SMOTE:\n",
      "Accuracy: 0.9872\n",
      "Precision: 0.1187\n",
      "Recall: 0.9238\n",
      "F1 Score: 0.2104\n",
      "Confusion Matrix:\n",
      "[[56137   720]\n",
      " [    8    97]]\n",
      "\n",
      "With Downsampling:\n",
      "Accuracy: 0.9676\n",
      "Precision: 0.0516\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.0979\n",
      "Confusion Matrix:\n",
      "[[55019  1838]\n",
      " [    5   100]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Without SMOTE:\")\n",
    "metrics = evaluate(y_test, y_pred)\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics['confusion_matrix'])\n",
    "\n",
    "print(\"\\nWith SMOTE:\")\n",
    "metrics_smote = evaluate(y_test, y_pred_smote)\n",
    "print(f\"Accuracy: {metrics_smote['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics_smote['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_smote['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics_smote['f1']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics_smote['confusion_matrix'])\n",
    "\n",
    "print(\"\\nWith Downsampling:\")\n",
    "metrics_down = evaluate(y_test, y_pred_down)\n",
    "print(f\"Accuracy: {metrics_down['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics_down['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_down['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics_down['f1']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics_down['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73f12dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.49\n"
     ]
    }
   ],
   "source": [
    "def find_best_threshold(model, X, y, thresholds=np.arange(0.1, 0.5, 0.01)):\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (model.predict_proba(X) >= threshold).astype(int)\n",
    "        metrics = evaluate(y, y_pred)\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "best_threshold = find_best_threshold(model, X_train, y_train)\n",
    "print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "y_pred_tuned = (model.predict_proba(X_test) >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e241251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

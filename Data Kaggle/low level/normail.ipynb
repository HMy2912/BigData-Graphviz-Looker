{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bdc6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\029at\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../creditcard.csv/creditcard.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['Time', 'Class'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3efad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the 'Amount' feature manually\n",
    "def standardize_feature(feature):\n",
    "    mean = np.mean(feature)\n",
    "    std = np.std(feature)\n",
    "    standardized = (feature - mean) / std\n",
    "    return standardized, mean, std\n",
    "\n",
    "# Apply standardization\n",
    "X['Amount'], amount_mean, amount_std = standardize_feature(X['Amount'].values)\n",
    "\n",
    "# To apply this standardization to new data later:\n",
    "# new_data = (new_data - amount_mean) / amount_std\n",
    "# Standardize the 'Amount' feature\n",
    "# scaler = StandardScaler()\n",
    "# X['Amount'] = scaler.fit_transform(X['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4511de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Split Data (Random, Not Stratified)\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(y) * (1 - test_size))\n",
    "    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "    return X.iloc[train_idx].values, X.iloc[test_idx].values, y[train_idx], y[test_idx]\n",
    "\n",
    "# Perform the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Convert X_train back to DataFrame after splitting\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1c02f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([ 60168,  37881,  64049,  44578,  99875, 140860,  13354, 135213,  13929,\\n       220859,\\n       ...\\n       137833,  56282,   5106,  84292,  17345,  43475,  37153, 196319, 173701,\\n        45516],\\n      dtype='int64', length=774)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(new_indices)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X[new_indices], y[new_indices]\n\u001b[1;32m---> 17\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mdownsample\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m new_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([fraud_indices, non_fraud_indices])\n\u001b[0;32m     13\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(new_indices)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_indices\u001b[49m\u001b[43m]\u001b[49m, y[new_indices]\n",
      "File \u001b[1;32mc:\\Users\\029at\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\029at\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\029at\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([ 60168,  37881,  64049,  44578,  99875, 140860,  13354, 135213,  13929,\\n       220859,\\n       ...\\n       137833,  56282,   5106,  84292,  17345,  43475,  37153, 196319, 173701,\\n        45516],\\n      dtype='int64', length=774)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# %% Downsample the Majority Class (Non-Fraud)\n",
    "def downsample(X, y):\n",
    "    # Get indices of both classes\n",
    "    fraud_indices = np.where(y == 1)[0]\n",
    "    non_fraud_indices = np.where(y == 0)[0]\n",
    "\n",
    "    # Downsample non-fraud cases to match fraud cases\n",
    "    np.random.shuffle(non_fraud_indices)\n",
    "    non_fraud_indices = non_fraud_indices[:len(fraud_indices)]\n",
    "\n",
    "    # Combine indices & shuffle\n",
    "    new_indices = np.concatenate([fraud_indices, non_fraud_indices])\n",
    "    np.random.shuffle(new_indices)\n",
    "\n",
    "    return X[new_indices], y[new_indices]\n",
    "\n",
    "X_train, y_train = downsample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868dab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term (column of 1s)\n",
    "X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eef8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"Compute the logistic regression cost function\"\"\"\n",
    "    h = sigmoid(X @ theta)\n",
    "    cost = (-y @ np.log(h)) - ((1 - y) @ np.log(1 - h))\n",
    "    return np.mean(cost)\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"Make predictions using learned theta\"\"\"\n",
    "    return (sigmoid(X @ theta) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3650607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights (important for imbalanced data)\n",
    "neg = np.sum(y_train == 0)\n",
    "pos = np.sum(y_train == 1)\n",
    "total = neg + pos\n",
    "\n",
    "# The weighting here means we'll count each positive example 100x more\n",
    "# to compensate for the class imbalance\n",
    "weight_for_0 = (1 / pos) * (total / 2.0)\n",
    "weight_for_1 = (1 / neg) * (total / 2.0)\n",
    "\n",
    "# Create sample weights\n",
    "sample_weights = np.ones(y_train.shape)\n",
    "sample_weights[y_train == 0] = weight_for_0\n",
    "sample_weights[y_train == 1] = weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01  # learning rate\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified gradient descent function to handle weights\n",
    "def gradient_descent(X, y, theta, alpha, num_iters, sample_weights=None):\n",
    "    \"\"\"Perform gradient descent to learn theta with optional sample weights\"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        h = sigmoid(X @ theta)\n",
    "        if sample_weights is not None:\n",
    "            gradient = (X.T @ ((h - y) * sample_weights)) / m\n",
    "        else:\n",
    "            gradient = X.T @ (h - y) / m\n",
    "        theta -= alpha * gradient\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return theta, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with weighted gradient descent\n",
    "theta, cost_history = gradient_descent(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    theta, \n",
    "    alpha, \n",
    "    iterations,\n",
    "    sample_weights=sample_weights  # Modified gradient descent to include weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = predict(X_test, theta)\n",
    "\n",
    "# Since data is imbalanced, don't use accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC AUC Score:\", roc_auc_score(y_test, sigmoid(X_test @ theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_scores = sigmoid(X_test @ theta)\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.title(\"Precision-Recall Tradeoff\")\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal threshold (example: maximize F1-score)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Make predictions with optimal threshold\n",
    "y_pred_optimal = (y_scores >= optimal_threshold).astype(int)\n",
    "\n",
    "print(\"\\nOptimized Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance (absolute value of coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': ['Intercept'] + list(data.drop(['Time', 'Class'], axis=1).columns),\n",
    "    'Coefficient': theta\n",
    "})\n",
    "\n",
    "# Sort by absolute value\n",
    "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c387399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

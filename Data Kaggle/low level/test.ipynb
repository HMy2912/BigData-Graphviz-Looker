{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19cfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import math\n",
    "import time\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56b188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Improved Data Loading and Parsing\n",
    "def load_and_parse_data(sc, filepath):\n",
    "    \"\"\"Load and parse CSV data, handling headers and malformed records\"\"\"\n",
    "    try:\n",
    "        # More robust header handling\n",
    "        lines = sc.textFile(filepath)\n",
    "        # header = lines.first()\n",
    "\n",
    "        indexed_rdd = lines.zipWithIndex()\n",
    "\n",
    "        header = indexed_rdd.filter(lambda x: x[1] == 0).map(lambda x: x[0]).collect()\n",
    "        if header:\n",
    "            header = header[0]\n",
    "            data = indexed_rdd.filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "        \n",
    "        # Skip header and parse data\n",
    "        data = lines.filter(lambda line: line != header).map(\n",
    "            lambda line: [float(x.strip('\"')) if x.strip('\"').isdigit() else 0.0 \n",
    "            for x in line.split(\",\")]\n",
    "        )\n",
    "        \n",
    "        # Create feature-label pairs, handle empty lines\n",
    "        rdd_data = data.filter(lambda cols: len(cols) > 1).map(\n",
    "            lambda cols: (cols[:-1], cols[-1])\n",
    "        )\n",
    "        \n",
    "        # Cache as we'll reuse this RDD\n",
    "        rdd_data.cache()\n",
    "        \n",
    "        # Count features for verification\n",
    "        num_features = len(rdd_data.first()[0]) if not rdd_data.isEmpty() else 0\n",
    "        print(f\"Loaded dataset with {rdd_data.count()} records and {num_features} features\")\n",
    "        \n",
    "        return rdd_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return sc.emptyRDD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f6d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 284807 records and 30 features\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = \"../creditcard.csv/creditcard.csv\"\n",
    "rdd_data = load_and_parse_data(sc, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f56b9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad1b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Enhanced Initialization with Feature Scaling\n",
    "def feature_scaling(rdd):\n",
    "    \"\"\"Scale features to zero mean and unit variance\"\"\"\n",
    "    # Calculate stats for each feature\n",
    "    feature_stats = rdd.map(lambda x: x[0]).zipWithIndex().flatMap(\n",
    "        lambda x: [(i, (val, val**2, 1)) for i, val in enumerate(x[0])]\n",
    "    ).reduceByKey(\n",
    "        lambda a, b: (a[0]+b[0], a[1]+b[1], a[2]+b[2])\n",
    "    ).collectAsMap()\n",
    "    \n",
    "    # Calculate mean and std for each feature\n",
    "    scaling_params = {}\n",
    "    for i, (sum_x, sum_x2, count) in feature_stats.items():\n",
    "        mean = sum_x / count\n",
    "        std = math.sqrt((sum_x2 / count) - mean**2)\n",
    "        scaling_params[i] = (mean, std if std != 0 else 1.0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_rdd = rdd.map(\n",
    "        lambda x: (\n",
    "            [(val - scaling_params[i][0])/scaling_params[i][1] for i, val in enumerate(x[0])],\n",
    "            x[1]\n",
    "        )\n",
    "    )\n",
    "    return scaled_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a35954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for gradient descent)\n",
    "scaled_data = feature_scaling(rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebbae71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ac4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    first_record = rdd_data.first()\n",
    "except Exception as e:\n",
    "    print(f\"Error getting first element: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d291ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_record[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965c3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "num_features = len(first_record[0])\n",
    "initial_weights = [0.0] * num_features\n",
    "learning_rate = 0.1  # Can be larger with scaled features\n",
    "num_iterations = 50\n",
    "regularization_param = 0.01  # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5cede4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Enhanced Helper Functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid function\"\"\"\n",
    "    # Prevent overflow\n",
    "    z = max(min(z, 20), -20)\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "def predict(features, weights):\n",
    "    \"\"\"Compute prediction with bias term\"\"\"\n",
    "    # Add bias term (1.0) to features\n",
    "    extended_features = features + [1.0]\n",
    "    extended_weights = weights + [0.0]  # Bias term weight\n",
    "    z = sum(w * f for w, f in zip(extended_weights, extended_features))\n",
    "    return sigmoid(z)\n",
    "\n",
    "def compute_gradient(point, weights):\n",
    "    \"\"\"Compute gradient with regularization\"\"\"\n",
    "    features, label = point\n",
    "    prediction = predict(features, weights)\n",
    "    error = prediction - label\n",
    "    \n",
    "    # Gradient for features\n",
    "    gradient = [error * f for f in features]\n",
    "    \n",
    "    # Add regularization (excluding bias term)\n",
    "    gradient = [g + regularization_param * w for g, w in zip(gradient, weights)]\n",
    "    \n",
    "    # Gradient for bias term (always 1.0)\n",
    "    bias_gradient = error\n",
    "    \n",
    "    return gradient + [bias_gradient]\n",
    "\n",
    "def compute_loss(point, weights):\n",
    "    \"\"\"Compute regularized logistic loss\"\"\"\n",
    "    features, label = point\n",
    "    prediction = predict(features, weights)\n",
    "    \n",
    "    # Avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    prediction = max(min(prediction, 1 - epsilon), epsilon)\n",
    "    \n",
    "    # Log loss\n",
    "    loss = -label * math.log(prediction) - (1 - label) * math.log(1 - prediction)\n",
    "    \n",
    "    # L2 regularization\n",
    "    reg_loss = 0.5 * regularization_param * sum(w**2 for w in weights)\n",
    "    \n",
    "    return loss + reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d168ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Enhanced Training with Early Stopping\n",
    "def train_logistic_regression(rdd, initial_weights, learning_rate, max_iter):\n",
    "    \"\"\"Train logistic regression with early stopping\"\"\"\n",
    "    weights = initial_weights.copy()\n",
    "    best_weights = initial_weights.copy()\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Compute gradients (map-reduce)\n",
    "        gradients = rdd.map(\n",
    "            lambda point: compute_gradient(point, weights)\n",
    "        ).reduce(\n",
    "            lambda a, b: [x + y for x, y in zip(a, b)]\n",
    "        )\n",
    "        \n",
    "        # Average gradients\n",
    "        num_points = rdd.count()\n",
    "        gradients = [g / num_points for g in gradients]\n",
    "        \n",
    "        # Update weights\n",
    "        weights = [w - learning_rate * g for w, g in zip(weights, gradients)]\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss = rdd.map(\n",
    "            lambda point: compute_loss(point, weights)\n",
    "        ).sum()\n",
    "        avg_loss = total_loss / num_points\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_weights = weights.copy()\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= 5:\n",
    "                print(f\"Early stopping at iteration {i}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        iteration_time = time.time() - start_time\n",
    "        print(f\"Iteration {i}: Loss = {avg_loss:.6f}, Time = {iteration_time:.2f}s\")\n",
    "    \n",
    "    return best_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fcaf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Iteration 0: Loss = 0.668642, Time = 16.65s\n",
      "Iteration 1: Loss = 0.645371, Time = 15.80s\n",
      "Iteration 2: Loss = 0.623270, Time = 15.63s\n",
      "Iteration 3: Loss = 0.602279, Time = 17.83s\n",
      "Iteration 4: Loss = 0.582338, Time = 16.19s\n",
      "Iteration 5: Loss = 0.563391, Time = 19.88s\n",
      "Iteration 6: Loss = 0.545384, Time = 15.47s\n",
      "Iteration 7: Loss = 0.528266, Time = 15.40s\n",
      "Iteration 8: Loss = 0.511987, Time = 14.96s\n",
      "Iteration 9: Loss = 0.496502, Time = 15.49s\n",
      "Iteration 10: Loss = 0.481766, Time = 15.57s\n",
      "Iteration 11: Loss = 0.467736, Time = 15.34s\n",
      "Iteration 12: Loss = 0.454375, Time = 15.23s\n",
      "Iteration 13: Loss = 0.441644, Time = 15.54s\n",
      "Iteration 14: Loss = 0.429508, Time = 15.21s\n",
      "Iteration 15: Loss = 0.417935, Time = 15.67s\n",
      "Iteration 16: Loss = 0.406892, Time = 15.60s\n",
      "Iteration 17: Loss = 0.396351, Time = 15.60s\n",
      "Iteration 18: Loss = 0.386284, Time = 15.10s\n",
      "Iteration 19: Loss = 0.376666, Time = 15.80s\n",
      "Iteration 20: Loss = 0.367470, Time = 15.20s\n",
      "Iteration 21: Loss = 0.358676, Time = 15.67s\n",
      "Iteration 22: Loss = 0.350261, Time = 15.77s\n",
      "Iteration 23: Loss = 0.342204, Time = 16.12s\n",
      "Iteration 24: Loss = 0.334488, Time = 15.88s\n",
      "Iteration 25: Loss = 0.327093, Time = 17.37s\n",
      "Iteration 26: Loss = 0.320003, Time = 15.93s\n",
      "Iteration 27: Loss = 0.313203, Time = 15.42s\n",
      "Iteration 28: Loss = 0.306677, Time = 15.49s\n",
      "Iteration 29: Loss = 0.300412, Time = 15.62s\n",
      "Iteration 30: Loss = 0.294394, Time = 15.78s\n",
      "Iteration 31: Loss = 0.288611, Time = 14.98s\n",
      "Iteration 32: Loss = 0.283051, Time = 15.84s\n",
      "Iteration 33: Loss = 0.277703, Time = 16.33s\n",
      "Iteration 34: Loss = 0.272558, Time = 15.86s\n",
      "Iteration 35: Loss = 0.267604, Time = 16.73s\n",
      "Iteration 36: Loss = 0.262834, Time = 16.87s\n",
      "Iteration 37: Loss = 0.258238, Time = 15.93s\n",
      "Iteration 38: Loss = 0.253808, Time = 15.93s\n",
      "Iteration 39: Loss = 0.249536, Time = 15.84s\n",
      "Iteration 40: Loss = 0.245416, Time = 15.95s\n",
      "Iteration 41: Loss = 0.241440, Time = 15.87s\n",
      "Iteration 42: Loss = 0.237602, Time = 16.79s\n",
      "Iteration 43: Loss = 0.233896, Time = 17.10s\n",
      "Iteration 44: Loss = 0.230315, Time = 17.15s\n",
      "Iteration 45: Loss = 0.226854, Time = 16.17s\n",
      "Iteration 46: Loss = 0.223509, Time = 15.47s\n",
      "Iteration 47: Loss = 0.220274, Time = 16.16s\n",
      "Iteration 48: Loss = 0.217144, Time = 16.11s\n",
      "Iteration 49: Loss = 0.214115, Time = 16.53s\n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "final_weights = train_logistic_regression(\n",
    "    scaled_data, \n",
    "    initial_weights + [0.0],  # Add bias term\n",
    "    learning_rate, \n",
    "    num_iterations\n",
    ")\n",
    "print(\"\\nTraining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d734486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Comprehensive Evaluation\n",
    "def evaluate_model(rdd, weights, threshold=0.5):\n",
    "    \"\"\"Evaluate model with multiple metrics\"\"\"\n",
    "    # Make predictions\n",
    "    predictions = rdd.map(\n",
    "        lambda point: (\n",
    "            predict(point[0], weights),  # Probability\n",
    "            point[1]  # Actual label\n",
    "        )\n",
    "    ).cache()\n",
    "    \n",
    "    # Calculate metrics at different thresholds\n",
    "    results = {}\n",
    "    for threshold in [0.3, 0.5, 0.7]:\n",
    "        # Classify based on threshold\n",
    "        classified = predictions.map(\n",
    "            lambda x: (1 if x[0] >= threshold else 0, x[1])\n",
    "        )\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        true_pos = classified.filter(lambda x: x[0] == 1 and x[1] == 1).count()\n",
    "        false_pos = classified.filter(lambda x: x[0] == 1 and x[1] == 0).count()\n",
    "        true_neg = classified.filter(lambda x: x[0] == 0 and x[1] == 0).count()\n",
    "        false_neg = classified.filter(lambda x: x[0] == 0 and x[1] == 1).count()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (true_pos + true_neg) / (true_pos + false_pos + true_neg + false_neg)\n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results[threshold] = {\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': {\n",
    "                'true_pos': true_pos,\n",
    "                'false_pos': false_pos,\n",
    "                'true_neg': true_neg,\n",
    "                'false_neg': false_neg\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e499d9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "\n",
      "Metrics at threshold 0.3:\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 284315\n",
      "False Negatives: 492\n",
      "\n",
      "Metrics at threshold 0.5:\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 284315\n",
      "False Negatives: 492\n",
      "\n",
      "Metrics at threshold 0.7:\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Confusion Matrix:\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 284315\n",
      "False Negatives: 492\n",
      "\n",
      "Final weights (including bias term):\n",
      "[-0.001659404997448309, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0005283581234271225, -1.506893951529394]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_results = evaluate_model(scaled_data, final_weights)\n",
    "\n",
    "for threshold, metrics in eval_results.items():\n",
    "    print(f\"\\nMetrics at threshold {threshold}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"True Positives: {metrics['confusion_matrix']['true_pos']}\")\n",
    "    print(f\"False Positives: {metrics['confusion_matrix']['false_pos']}\")\n",
    "    print(f\"True Negatives: {metrics['confusion_matrix']['true_neg']}\")\n",
    "    print(f\"False Negatives: {metrics['confusion_matrix']['false_neg']}\")\n",
    "\n",
    "# Save final weights\n",
    "print(\"\\nFinal weights (including bias term):\")\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3ea02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
